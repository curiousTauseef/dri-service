\chapter{Introduction}
\label{cha:introduction}

This chapter presents the overview and general objectives of this thesis.
The first section outlines the background and the concept of
assuring data reliability. Cloud storage availability and integrity challenges
are introduced in the following section. Next, we present the origin of
this work in the context of VPH-Share Cloud Platform. In the final section,
the high-level objectives of this thesis are described.
 
\section{Background and overview}
In the modern world of the 21st century the data and its vast volume
are ubiquitous. The total amount of global data stored to date is estimated
as 4 zettabytes ($4 \times 10^{21}bytes$) in 2013, almost $50\%$ more than in 2012 \cite{idc-data-growth}.
As long as computers spread to 
new domains and new computing paradigms -- as internet of things and big data
-- become a reality, the trend of exponential growth of data volume
will continue. The main sources of data are of various kind:

\begin{itemize}
\item \textbf{personal data} -- generated by and associated with people such as
images, videos, emails, documents etc, stored on privately held devices as
laptops, smartphones or digital cameras as well as by website owners in
big data centers,
\item \textbf{business data} -- generated by companies and corporations
that enables them to run and maintain their daily business,
\item \textbf{experimental data} -- generated by all kind of sensor and
experimental devices from weather stations, to particle accelerators, to
space satellites and stored on academic resources and scientific data centers.
\end{itemize}

It is an engineering challenge to fullfill storage requirements for current
data growth. Nowadays, we observe a rapid shift from privately owned and maintained
computer systems toward virtualized computer infrastructures provided as a service,
namely cloud computing. At least several commercial cloud infrastructure offerings 
provide access to virtualized computer systems at different level -- Infrastructure
as a Service (IaaS), Platform as a Service (PaaS) and Software as a Service (SaaS).
Low costs, high availability and scalability, decent performance degradation and 
seamless integration are often mentioned as major benefits by cloud adopters and
evangelists.\\

Digitalization brought new opportunities and advantages to the business and communities.
As a~result, many of them became strictly tied to their data as a major asset and
can no longer tolerate any data loss. Temporal data unavailability can also pose 
a~problem in domains such as medical care and flight services. With the promise
of scalable data storage, where everything can be stored or archived for future
needs, it is a burning issue to provide means of data loss prevention.\\

In general, the methods of data loss prevention can be divided into two groups:

\begin{enumerate}
\item \textbf{corruption or loss detection} -- that the data content is unavailable
or no longer correct,
\item \textbf{corruption or loss recovery} -- that after data corruption detection
we can recover its original properties of availability and integrity.
\end{enumerate}

A number of methods exist and are in widespread use nowadays. Storage hardware
solutions -- CDs, hard disks, etc -- utilize error correcting codes (ECC) to prevent
small scale errors on read/write operations. Network protocols and software packages
distribution use hash-based checksums to verify the integrity of the content. Data
backups and replication are universal way of enabling recovery from data loss.\\

Ideally, from the user point of view, data storage solution should provide highly
available and fault-tolerant access to the data and be free of data corruption and
unavailability problems. However, it often appears that the above-mentioned
criteria cannot be met in practice,
especially when relying on single service provider. Cloud service providers protect
their customers data with data replication to geographically distributed zones,
strong securitiy policies and infrastructure monitoring. Even though, recently
a~number of cloud failures occured, questioning the reliability of
cloud solutions \cite{cloud-downtime-stats} . Malicious and accidental data corruption threats also require
attention.\\ 

The standard concept of checking data integrity is based on checksum verification.
It consists of two major steps -- initial setup and verification. The first step
concerns with initial data deployment and computing checksum metadata of the content
-- a hash.In the verification step the integrity checksums of the data are once more
computed and compared with the reference ones. In the scope of this thesis, we discuss
the means of providing data reliability, by which we mean both:

\begin{enumerate}
\item \textbf{availability} -- that the data is available to the requesting entity,
\item \textbf{integrity} -- that the data remains untouched by malicious or undesired
modifications.
\end{enumerate}

Managing data availability and integrity of data in cloud storage environment is the
subject of this thesis. As we mentioned above, to provide a solid way of assuring data
reliability we must address both data corruption detection and recovery.

\section{Cloud storage data reliability challenges}

Over the last years, we observe a rapid shift from privately owned and maintained
storage resources toward cloud storage solutions \cite{cisco-cloud-shift}. The cloud model of business become
popular and adopted by many organizations. The main driving forces of cloud computing
shift are low costs, high availability and scalability, decent performance degradation
and seamless integration. While the emerging trend determines a significant step forward 
in storage technology and brings a lot of advantages as data replication, no administration
costs, pay-as-you-use, SLA contracts -- it can appear challenging for ensuring data 
reliability. Remotely available resources introduce network transfer rates and latency issues,
while SLA contracts are mostly best-effort -- cloud provider will not charge fees if service
quality has not been met. Additionally, recent cloud storage failures or security break
reports shown that we cannot entrust our data to cloud providers entirely.\\
 
Classic checksum-based integrity verification methods are based on the whole content of data. It
poses a challenge to efficiently verify the integrity of vast volumes of data stored on remote resources,
where network transfer rates and latency comes into play. Additionally, cloud
storage providers charge fees not only for storage space used, but also for storage transfer, especially outbound transfer.
Consequently, cloud storage data reliability methods should take these limitations into
consideration. It appears inevitable that cloud storage data integrity can be only provided with
some level of probability and has to be based on a fraction of the file. A part of this thesis main
objective is to select and implement a~network-efficient method of ensuring data reliability.


\section{VPH-Share Cloud Platform context}
This thesis originates as a part of the VPH-Share project founded by European Commission
which brings together twenty international partners from academia, industry and healthcare,
led by University of Sheffield \cite{vph-share-website}. Its main goal is to build a collaborative computing
environment and infrastructure where researchers from the domain of physiopathology of
the human body will work together on developing new medical simulation software. The inspiring
vision is to create a versatile environment for sharing of information -- tools, models and data
-- to work efficiently towards building a complete model of the human body.\\

\subsection{Basic architecture}
The project has layered architecture divided into work packages distributed among consortium
members (see figure \ref{fig:dri-high-level}). The design is based on cloud computing middleware
-- a hybrid of commercial and private resources on top of hardware layer. Data and Compute
Cloud Platform is one of the main building blocks of the VPH-Share project. Its goal is to
develop and integrate a consistent service-based cloud infrastructure that will enable VPH
community to deploy basic components of VPH-Share application workflows (known as Atomic Services)
on the available computing resources and then enact workflows using these services. Access to the
services layer will be provided to system users through user interface(UI).

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{images/vph-high-level.png}
	\caption{VPH-Share overview\\
	The project aims to build a collaborative computing environment
	for researchers of human body to work on developing new medical simulation software.
	Its design has layered architecture centered around service-based Data and Compute
	Cloud Platform built on top of hybrid cloud middleware, both commercial and private. The VPH-Share
	users use the platform through common user interface layer \cite{vph-deliverable-2-2}}
	\label{fig:dri-high-level}
\end{figure} 


VPH-Share specifies three groups of users:
application providers, domain scientists and system administrators \cite{vph-deliverable-2-2}. Application providers
are responsible for developing and installing scientific applications and software packages.
Domain scientists are actual researchers of VPH community who will use and benefit from 
the platform. Finally, system administrators is a group of priviledged users who will
manage platform's hardware resources and will administer and maintain it.\\

According to the platform's design, data will be stored on federated cloud storage resources --
both commercial and private -- and available via common access layer \cite{vph-deliverable-2-2}. It is foreseen that stored
data volumes will be significant, but predominantly of static nature -- upon upload its content will
remain untouched. Additional measures should ensure data availability and integrity.As a result, two
of the key project's requirements regarding data storage and integrity are:

\begin{enumerate}
\item \textbf{access to large binary data in the cloud} -- specified groups of users will
be able to query for and store binary data uploaded or generated by workflows within the platform, 
\item \textbf{data reliability and integrity} -- platform users will be able to tag datasets
for automatic availability and integrity monitoring, set validation and replication policies,
as well as receive notifications about data integrity violations.
\end{enumerate}

VPH Cloud Platform puts strong empasis on data storage and availability and
integrity assurance, as it is mostly static medical data of great importance.
To fullfill this goal, VPH-Share bases on outsourcing data storage to multiple cloud storage
providers -- federated cloud storage. Data replication across cloud providers will
build an~abstraction layer on top of cloud services and will allow to use them interchangeably
in case of cloud provider failure. Additionally, VPH-Share platform will periodically monitor
the data availability and integrity of data and enable a~possibility to restore corrupted
entities from existing replicas. In case of irreparable corruption the owner should be notified
about the problem.\\

\subsection{Use cases and requirements}
\label{requirements}

From the point of view of VPH-Share user a~couple of crucial use cases regarding data availability
and integrity can be identified:

\begin{itemize}
	\item user tags a~specific dataset (a~set of files) for periodical data validation,
	\item user requests dataset validation,
	\item user requests dataset replication to the other cloud provider,
	\item user gets notified if data validation, periodical or on-request, discovered data
	unavailability or corruption.
\end{itemize}

The above use cases are provided by separate data reliability and integrity (DRI) component
in service-based VPH-Share architecture. The formal functional and nonfunctional requirements
of DRI are presented below.

\subsubsection*{Functional}
Functional requirements are related to the core DRI capabilities that it
is desired to ensure:

\begin{itemize}
	\item \textbf{Periodic and on-request validation}: DRI has to periodically
	fetch datasets' metadata and check the availability and integrity of managed
	datasets. It also has to enable API interface for this operation to be invoked
	by the user on demand.
	\item \textbf{Data replication}: DRI has to enable API interface for data
	replication from one data source to the other. 
	\item \textbf{User notification about integrity errors}: when DRI will discover data
	unavailability or corruption it should notify the owner about the identified problems.
\end{itemize}

\subsubsection*{Nonfunctional}
Nonfunctional requirements are related to the quality of the core DRI capabilities:
\begin{itemize}
	\item \textbf{Network-efficient validation mechanism}: As it was shown, naive
	whole file content validation seems infeasible in case of cloud storage of vast volumes
	of data. As a~result, DRI should perform data validation efficiently from the perspective
	of network bandwidth, limiting the size of data that has to be downloaded to guarantee acceptable
	level of error detection.
	\item \textbf{Scalablility}: as it is foreseen that the amount of data
	stored in Cloud Platform resources will be significant, the DRI has to
	present the ability to scale with the size of data. It is suggested to be
	achieved by deploying many independent DRI replicas. 
	\item \textbf{Configurability}: DRI has to provide API interface or UI
	portlet to configure its most important parameters regarding data validation.
\end{itemize}


\section{Objectives of this work}

As it was mentioned in the previous sections, there is a~need to propose a~method to manage
data reliability and integrity in federated cloud storage, in particular within VPH-Share plaftorm
environment. In this thesis, we present data reliability and integrity (DRI) component that monitors
the availability and integrity of data. The high-level objective of this thesis is to design and
implement such component that:

\begin{enumerate}
\item efficiently and  periodically monitors the integrity of the federated cloud storage,
\item notifies the user about detected data corruption in advance of data retrieval, 
\item provides possibility to restore corrupted data from replicas in other cloud providers.
\end{enumerate}

In the scope of this thesis, by efficient data validation we mean network efficiency. Our goal is to
minimize network overhead incured against data source. While standard whole file content validation
is practically infeasible when considering external storage and its vast volumes, we aim to propose
an~algorithm that only requires to fetch a~fraction of file in order to detect data corruption on acceptable
level of probability. Additionally, DRI should be scalable and configurable to flexibly adjust its
network overhead.\\

Furthermore, we present a~proof of concept implementation of DRI component as a~part of service-based
VPH-Share Cloud Platform environment, which significantly influences its design. DRI considers a~concept
of dataset -- simply a set of files. Upon tagging dataset as managed DRI triggers
integrity checksums computation and stores them in metadata registry. As long as dataset remains managed
a~periodical availability and integrity verification takes place. When data corruption is detected the
user is notified about the errors via notification service and can restore the content from other replicas.


